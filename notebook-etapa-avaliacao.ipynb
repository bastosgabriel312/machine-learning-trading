{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoJAx-8mG3M"
      },
      "source": [
        "# Instalação de dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyw5AmYWmLQ7",
        "outputId": "7dfda9bd-75d0-4bcc-ca1a-3fca32191182"
      },
      "outputs": [],
      "source": [
        "!pip install python-binance "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KghDFEWznZkh",
        "outputId": "52d26208-1d93-4ac3-9b93-1acdfff54354"
      },
      "outputs": [],
      "source": [
        "import notebook\n",
        "notebook.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOMUwjeEmP5C"
      },
      "source": [
        "# **utils.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "322jg9rNltft"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from binance.client import Client\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# Recupera os dados históricos da binance e retorna um dataframe\n",
        "def retrieve_historical_data_binance(client, symbol, interval, limit):\n",
        "    klines = client.get_klines(symbol=symbol, interval=interval, limit=limit)\n",
        "    df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
        "                                       'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume',\n",
        "                                       'taker_buy_quote_asset_volume', 'ignore'])\n",
        "    df['close'] = df['close'].astype(float)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_klines_df(client, symbol, interval, total_periods=2000):\n",
        "    klines_df = pd.DataFrame(columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])\n",
        "\n",
        "    periods_received = 0\n",
        "    last_timestamp = None\n",
        "\n",
        "    while periods_received < total_periods:\n",
        "        klines = client.get_klines(\n",
        "            symbol=symbol,\n",
        "            interval=interval,\n",
        "            limit=total_periods - periods_received,\n",
        "            endTime=last_timestamp\n",
        "        )\n",
        "\n",
        "        if not klines:\n",
        "            break\n",
        "\n",
        "        for kline in klines:\n",
        "            klines_df.loc[len(klines_df)] = kline\n",
        "\n",
        "        periods_received += len(klines)\n",
        "\n",
        "        last_timestamp = klines[0][0]\n",
        "\n",
        "    # Convert timestamp column to datetime\n",
        "    klines_df['timestamp_to_date'] = pd.to_datetime(klines_df['timestamp'], unit='ms')\n",
        "\n",
        "    return klines_df.sort_values(by='timestamp')\n",
        "\n",
        "\n",
        "# Calcula manualmente a métrica de mvrv_ratio\n",
        "def retrieve_mvrv_ratio(df_binance):\n",
        "    df_binance['mvrv_ratio'] = df_binance['close'].astype(float) / ((df_binance['close'].astype(float) + df_binance['high'].astype(float)+ df_binance['low'].astype(float) + df_binance['open'].astype(float)) / 4.0)\n",
        "    return df_binance\n",
        "\n",
        "# Retorna o datafrime da Binance com o mvrv calculado\n",
        "def get_concatenate_klines_mvrv(client, symbol, interval, limit):\n",
        "    df_binance_klines = get_klines_df(client, symbol, interval, limit)\n",
        "\n",
        "    #from_date,to_date = timestamp_to_datestring(df_binance_klines['timestamp'].iloc[-0]), timestamp_to_datestring(df_binance_klines['timestamp'].iloc[-1])\n",
        "    df_binance_klines['timestamp'] = df_binance_klines['timestamp'] // 1000\n",
        "    df_binance_klines = retrieve_mvrv_ratio(df_binance_klines)\n",
        "    return df_binance_klines\n",
        "\n",
        "# Função para calcular os quartis em uma coluna de DataFrame\n",
        "def calculate_quartiles(df, column_name):\n",
        "    \"\"\"\n",
        "    Calcula os quartis (25%, 50%, 75%) em uma coluna de um DataFrame.\n",
        "\n",
        "    :param df: DataFrame\n",
        "    :param column_name: Nome da coluna\n",
        "    :return: Um dicionário com os valores dos quartis\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"A coluna '{column_name}' não existe no DataFrame.\")\n",
        "    df[column_name] = df[column_name].astype('float64')\n",
        "    values = df[column_name].values\n",
        "    first_quartile = np.percentile(values, 25)\n",
        "    second_quartile = np.percentile(values, 50)\n",
        "    third_quartile = np.percentile(values, 75)\n",
        "\n",
        "    return {\n",
        "        'first_quartile': first_quartile,\n",
        "        'second_quartile': second_quartile,\n",
        "        'third_quartile': third_quartile\n",
        "    }\n",
        "\n",
        "\n",
        "# Função para adicionar colunas indicando quartis a um DataFrame\n",
        "def add_quartile_columns(df, column_name):\n",
        "    \"\"\"\n",
        "    Adiciona colunas ao DataFrame indicando se cada valor está no primeiro, segundo ou terceiro quartil.\n",
        "\n",
        "    :param df: DataFrame\n",
        "    :param column_name: Nome da coluna\n",
        "    :return: O DataFrame com as novas colunas\n",
        "    \"\"\"\n",
        "    quartiles = calculate_quartiles(df, column_name)\n",
        "    first_quartile = quartiles['first_quartile']\n",
        "    second_quartile = quartiles['second_quartile']\n",
        "    third_quartile = quartiles['third_quartile']\n",
        "    df[f'{column_name}_in_first_quartile'] = (df[column_name] <= first_quartile).astype(int)\n",
        "    df[f'{column_name}_in_second_quartile'] = ((first_quartile < df[column_name]) & (df[column_name] <= second_quartile)).astype(int)\n",
        "    df[f'{column_name}_in_third_quartile'] = ((second_quartile < df[column_name]) & (df[column_name] <= third_quartile)).astype(int)\n",
        "    df.drop(column_name, axis=1)\n",
        "    return df\n",
        "\n",
        "# Função que transforma timestamp em data\n",
        "def timestamp_to_datestring(timestamp):\n",
        "    ts = timestamp/ 1000\n",
        "    data = datetime.fromtimestamp(ts)\n",
        "    return data.strftime('%Y-%m-%d')\n",
        "\n",
        "# Função utilizada para normalizar uma coluna\n",
        "def normalize_data(df, column_name):\n",
        "  # Inicialize o MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  # Ajuste o scaler aos dados\n",
        "  scaler.fit(df[[column_name]])\n",
        "\n",
        "  # Aplique a transformação aos dados\n",
        "  return scaler.transform(df[[column_name]])\n",
        "\n",
        "def preprocess_manual(historical_data):\n",
        "    # Adiciona a coluna que no dataset será o alvo\n",
        "    historical_data = calcular_target_high(historical_data.copy())\n",
        "\n",
        "    # Lista das colunas a serem normalizadas\n",
        "    list_columns_normalize = [\n",
        "        'volume', 'quote_asset_volume', 'number_of_trades',\n",
        "        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume',\n",
        "        'open', 'high', 'low', 'close', 'mvrv_ratio'\n",
        "    ]\n",
        "\n",
        "    # Crie um dicionário para armazenar os scalers\n",
        "    scalers = {}\n",
        "    # Loop para normalizar cada coluna e armazenar o scaler correspondente\n",
        "    for col in list_columns_normalize:\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(historical_data[[col]])  # Ajusta o scaler apenas a uma coluna\n",
        "        scalers[col] = scaler  # Armazena o scaler no dicionário\n",
        "\n",
        "        # Aplica a transformação à coluna no DataFrame\n",
        "        historical_data[[col]] = scaler.transform(historical_data[[col]])\n",
        "\n",
        "    # Cria um DataFrame temporário com as colunas normalizadas e nomes modificados\n",
        "    colunas_normalizadas = pd.DataFrame(\n",
        "        {f'normalized_{col}': historical_data[col] for col in list_columns_normalize}\n",
        "    )\n",
        "\n",
        "    # Adiciona a coluna alvo ao DataFrame temporário\n",
        "    colunas_normalizadas['target-high'] = historical_data['target-high']\n",
        "\n",
        "    # Substitui as colunas originais em 'historical_data' pelas colunas normalizadas com nomes modificados\n",
        "    historical_data = colunas_normalizadas\n",
        "\n",
        "    # Retorna o DataFrame preprocessado e o dicionário de scalers\n",
        "    return historical_data, scalers\n",
        "\n",
        "def calcular_target_high(historical_data, num_periodos=4):\n",
        "    max_highs = []\n",
        "\n",
        "    for i in range(len(historical_data)):\n",
        "        # calcula o valor máximo da coluna 'high' nos próximos períodos.\n",
        "        max_high = historical_data['high'].iloc[i:i + num_periodos].max()\n",
        "        max_highs.append(max_high)\n",
        "\n",
        "    historical_data['target-high'] = max_highs\n",
        "    historical_data_alvo = historical_data.copy()\n",
        "    historical_data_alvo['target-high'] = historical_data_alvo['target-high'].astype('float64')\n",
        "    return historical_data_alvo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjkX8kYNmV1T"
      },
      "source": [
        "# Lendo dados históricos e pré processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "m1iyT44CPQ82",
        "outputId": "8fed0543-ed79-46b5-e79c-9833bba2ebdd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/1_experimento/historical_data_ETCUSDT_10k.csv')\n",
        "\n",
        "###### PREPROCESSA OS DADOS\n",
        "dataset,scaler = preprocess_manual(df)\n",
        "COLUNAS_DATASET_COM_ALV0 = dataset.columns\n",
        "COLUNAS_DATASET_SEM_ALVO = dataset.copy().drop('target-high', axis=1).columns\n",
        "\n",
        "dataset = dataset.copy()\n",
        "\n",
        "# Separando as variáveis independentes (X) e a variável alvo (y)\n",
        "X = dataset.drop(['target-high'], axis=1)\n",
        "y = dataset['target-high']\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste\n",
        "random_state = 15\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=random_state)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hzuuazwmwUc"
      },
      "source": [
        "# Regressão Linear Multipla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXTdzu2DnFql"
      },
      "source": [
        "# RNA Multilayer Perceptron Regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaZnasmNUawz"
      },
      "outputs": [],
      "source": [
        "#RNA MLP REDE NEURAL MUTILAYER PERCEPTRON REGRESSOR (ALVO TARGET-HIGH)\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import preprocessing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)\n",
        "\n",
        "RNA = MLPRegressor(\n",
        "    hidden_layer_sizes=(3),\n",
        "    tol=0.0000001,\n",
        "    learning_rate_init=0.1,\n",
        "    verbose=0,\n",
        "    random_state=random_state,\n",
        "    validation_fraction = 0.2,\n",
        "    max_iter = 5000\n",
        "\n",
        ")\n",
        "\n",
        "RNA.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões com os dados de teste\n",
        "y_pred_RNA = RNA.predict(X_test)\n",
        "r2_RNA = r2_score(y_test, y_pred_RNA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4VZbmLUnRJ0"
      },
      "source": [
        "# Regressão de Árvore de Decisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8ZUlgXX66uF"
      },
      "outputs": [],
      "source": [
        "#REGRESSÃO DE ARVORE DE DECISAO (ALVO TARGET-HIGH)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Criando e treinando o modelo de árvore de decisão\n",
        "tree_reg = DecisionTreeRegressor(random_state=random_state)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "y_pred_tree = tree_reg.predict(X_test)\n",
        "\n",
        "# Calculando o erro médio quadrático (RMSE)\n",
        "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhdc9gHmndVS"
      },
      "source": [
        "# Regressão de Floresta Aleatória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKVGtvgx8m9D"
      },
      "outputs": [],
      "source": [
        "#REGRESSÃO DE FLORESTA ALEATÓRIA (ALVO TARGET-HIGH)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Criando e treinando o modelo de floresta aleatória\n",
        "rf_reg = RandomForestRegressor(random_state=random_state)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculando o erro médio quadrático (RMSE)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODcRxdono1n"
      },
      "source": [
        "# Regressão de Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IgkH7Kc9GG8",
        "outputId": "d8174ffc-7bd1-4de5-821a-8b36e3f04486"
      },
      "outputs": [],
      "source": [
        "#REGRESSÃO DE SUPPORT VECTOR MACHINE (ALVO TARGET-HIGH)\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Criando e treinando o modelo SVM\n",
        "svm_reg = SVR(kernel='linear')\n",
        "svm_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "y_pred_svm = svm_reg.predict(X_test)\n",
        "\n",
        "# Calculando o erro médio quadrático (RMSE)\n",
        "rmse_svm = np.sqrt(mean_squared_error(y_test, y_pred_svm))\n",
        "\n",
        "print(\"Máquinas de Vetores de Suporte (SVM) - RMSE:\", rmse_svm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JihmMAgunvtW"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbutwoy0NEOx",
        "outputId": "48e529b2-43e4-412a-a5fd-d81488979df2"
      },
      "outputs": [],
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Criando e treinando o modelo SVM\n",
        "knn = KNeighborsRegressor(n_neighbors=4)  # Você pode ajustar o número de vizinhos (k) conforme necessário\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Calculando o erro médio quadrático (RMSE)\n",
        "rmse_knn= np.sqrt(mean_squared_error(y_test, y_pred_svm))\n",
        "\n",
        "print(\"K-Nearest Neighbors (KNN) - RMSE:\", rmse_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jGTEORvn2MC"
      },
      "source": [
        "# Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPya-gcJ9Q0y",
        "outputId": "7a7a6a12-4563-4695-8117-fb9762b300ef"
      },
      "outputs": [],
      "source": [
        "#### AVALIAÇÃO\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "\n",
        "\n",
        "DICIONARIO_MODELOS_REGRESSÃO = {'Regressão Linear':lr,'Árvore de decisão':tree_reg,'Floresta Aleatória':rf_reg,'K-Nearest Neighbors (KNN)':knn, 'Máquinas de Vetores de Suporte (SVM)':svm_reg, \"RNA MLP\": RNA}\n",
        "\n",
        "\n",
        "\n",
        "# Função para calcular o ERRO MÉDIO ABSOLUTO\n",
        "def mae(y_true, predictions):\n",
        "    y_true, predictions = np.array(y_true).astype(float), np.array(predictions).astype(float)\n",
        "    return np.mean(np.abs(y_true - predictions))\n",
        "\n",
        "# Função para calcular as métricas e imprimir\n",
        "def calculate_metrics(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n",
        "    rmse_scores = np.sqrt(-scores)\n",
        "    mae_score = mae(y, y_pred)\n",
        "    return r2, mae_score, rmse_scores.mean()\n",
        "\n",
        "# DICIONARIO_MODELOS_REGRESSÃO contém os modelos\n",
        "\n",
        "best_model = None\n",
        "best_mean_r2 = -float('inf')\n",
        "best_mean_mae = float('inf')\n",
        "best_mean_rmse = float('inf')\n",
        "\n",
        "for model_name, model in DICIONARIO_MODELOS_REGRESSÃO.items():\n",
        "    r2, mae_score, mean_rmse = calculate_metrics(model, X_test, y_test)\n",
        "    print(f\"[MODELO: {model_name}]\")\n",
        "    print(f'R² Score: {r2:.4f}')\n",
        "    print(f'Erro Médio Absoluto (MAE): {mae_score:.4f}')\n",
        "    print(f'Validação Cruzada (RMSE):')\n",
        "    print(f'Média RMSE: {mean_rmse:.4f}')\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Verifique se este modelo é o melhor até agora com base nas métricas\n",
        "    if r2 > best_mean_r2 and mae_score < best_mean_mae and mean_rmse < best_mean_rmse:\n",
        "        best_model = model_name\n",
        "        best_mean_r2 = r2\n",
        "        best_mean_mae = mae_score\n",
        "        best_mean_rmse = mean_rmse\n",
        "\n",
        "print(f\"Melhor modelo: {best_model}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiaYXr1szhlM",
        "outputId": "6c49c232-e2d9-460c-fea6-acd43b54f063"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Defina seus modelos de regressão\n",
        "DICIONARIO_MODELOS_REGRESSÃO = {\n",
        "    'Regressão Linear': lr,\n",
        "    'Árvore de decisão': tree_reg,\n",
        "    'Floresta Aleatória': rf_reg,\n",
        "    'K-Nearest Neighbors (KNN)': knn,\n",
        "    'Máquinas de Vetores de Suporte (SVM)': svm_reg,\n",
        "    'RNA MLP REGRESSOR': RNA\n",
        "}\n",
        "\n",
        "# Crie listas vazias para armazenar os resultados\n",
        "modelos = []\n",
        "r2_scores = []\n",
        "media_rmse_scores = []\n",
        "mae_scores = []\n",
        "\n",
        "# Função para formatar um número com vírgula como separador decimal\n",
        "def formatar_com_virgula(numero):\n",
        "    return f'{numero:.4f}'.replace('.', ',')\n",
        "\n",
        "# Função para calcular a média RMSE a partir de scores\n",
        "def media_rmse(scores):\n",
        "    return formatar_com_virgula(np.sqrt(-scores).mean())\n",
        "\n",
        "# Loop através dos modelos e colete os resultados\n",
        "for model_name, model in DICIONARIO_MODELOS_REGRESSÃO.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    scores = cross_val_score(model, X_test, y_test, scoring='neg_mean_squared_error', cv=5)\n",
        "    rmse_mean = media_rmse(scores)\n",
        "    mae_score = formatar_com_virgula(mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "    modelos.append(model_name)\n",
        "    r2_scores.append(formatar_com_virgula(r2 * 100))  # Convertendo para percentual\n",
        "    media_rmse_scores.append(rmse_mean)\n",
        "    mae_scores.append(mae_score)\n",
        "\n",
        "# Crie um DataFrame com os resultados\n",
        "resultados_df = pd.DataFrame({\n",
        "    'Modelo': modelos,\n",
        "    'R² Score (%)': r2_scores,\n",
        "    'Média RMSE (Validação Cruzada)': media_rmse_scores,\n",
        "    'Erro Médio Absoluto (MAE)': mae_scores\n",
        "\n",
        "})\n",
        "\n",
        "# Exiba a tabela de resultados\n",
        "resultados_df\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sOMUwjeEmP5C"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
